{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3.7 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"training_notebook.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d4F2og35vvpP"},"source":[""],"id":"d4F2og35vvpP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d100df2b","executionInfo":{"status":"error","timestamp":1622285410809,"user_tz":-120,"elapsed":2363,"user":{"displayName":"Konrad Mueller","photoUrl":"","userId":"08068382991682809654"}},"outputId":"cc39f8c5-0deb-4024-88f3-65ad9d2c5eeb","colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["import numpy as np\n","from datetime import datetime\n","from tqdm import trange\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from env_total_v2 import Game\n","from utils import create_input_batch, save_weights, load_weights\n","\n","\n","log_dir = \"log/game/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","writer = tf.summary.create_file_writer(log_dir)\n","\n","# Hyperparameters\n","params = {\n","    \"epochs_single\": 1,  # number of epochs an agent is trained per training\n","    \"epochs_total\": 2,  # number of times each agent is trained\n","    \"steps\": 11,  # number of steps in each game\n","    \"population_size\": 5,  # number of different weights compared\n","    \"max_iterations\": 10,  # number of iterations optimization algorithm runs\n","    \"agent_num\": 2,\n","}\n","locals().update(params)\n","\n","IRM_POOL = [0.01, 0.02, 0.05]\n","IRS_POOL = [0.0]\n","DIVM_POOL = [0.5, 1, 1.5]\n","DIVS_POOL = [0.2, 0.3, 0.4]\n","ENDOW_CASH_POOL = [20, 100, 80]\n","ENDOW_STOCK_POOL = [10, 15, 10]\n","save = False\n","\n","# Batch of game scenarios\n","batch = create_input_batch(\n","    batch_size=epochs_single,\n","    agent_num=agent_num,\n","    steps=steps,\n","    irm_pool=IRM_POOL,\n","    divm_pool=DIVM_POOL,\n","    irs_pool=IRS_POOL,\n","    divs_pool=DIVS_POOL,\n","    endow_cash_pool=ENDOW_CASH_POOL,\n","    endow_stock_pool=ENDOW_STOCK_POOL,\n",")\n","\n","game = Game(steps, agent_num)\n","\n","# Dummy pass to ensure initialization\n","x = batch[0]\n","_ = game(x)\n","\n","\n","# Function to be optimized by differential evolution minimizer\n","def objective_fn(w1, b1, w2, b2):\n","    \"\"\"\n","    Wrapper function around game for training a specific agent\n","    Each input is tensor; first dimension indexes the elements in the population\n","    Each population element contains weights / biases of layers of agent's NN\n","    \"\"\"\n","    global iterations\n","    print(f\"Training iteration {iterations}\")\n","    iterations += 1\n","\n","    for i in trange(len(w1), unit=\"population_element\"):\n","\n","        game.first_layers[training_agent].set_weights([w1[i], b1[i]])\n","        game.out_layers[training_agent].set_weights([w2[i], b2[i]])\n","\n","        cumulative_reward = tf.Variable(tf.zeros(1))\n","        for epoch, game_inputs in enumerate(batch):\n","            _reward = game(game_inputs)[training_agent]\n","            cumulative_reward.assign_add(tf.expand_dims(_reward, 0))\n","\n","        if i == 0:\n","            cumulative_rewards = cumulative_reward\n","        else:\n","            cumulative_rewards = tf.concat([cumulative_rewards, cumulative_reward], 0)\n","\n","    return -cumulative_rewards  # minimize total negative rewards"],"id":"d100df2b","execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d166f4f028cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv_total_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env_total_v2'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"81d0d542","outputId":"1ea7fda9-6220-4371-c364-fe03abcdb3fe"},"source":["# Train all agents\n","print(\"---------------------------------------\")\n","print(\"Training output:\")\n","for k in np.arange(epochs_total * game.agent_num):\n","    iterations = 0  # Reset iterations\n","    training_agent = k % game.agent_num  # Agent to be trained\n","    if training_agent == 0:\n","        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","        print(\"EPOCHS_TOTAL: \" + str(int(k / game.agent_num)))\n","        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n","    print(\n","        \"####################### Training agent: \"\n","        + str(training_agent)\n","        + \" ######################\"\n","    )\n","\n","    # Initial weights of layers that are trained\n","    init_weights_first_layer = game.first_layers[training_agent].get_weights()\n","    init_weights_out_layer = game.out_layers[training_agent].get_weights()\n","    initial_position = [\n","        init_weights_first_layer[0],\n","        init_weights_first_layer[1],\n","        init_weights_out_layer[0],\n","        init_weights_out_layer[1],\n","    ]\n","\n","    # Train one agent\n","    optim_results = tfp.optimizer.differential_evolution_minimize(\n","        objective_fn,\n","        initial_position=initial_position,\n","        population_size=population_size,\n","        max_iterations=max_iterations,\n","        seed=0,\n","    )\n","\n","    # set weights of trained agent to best known\n","    game.first_layers[training_agent].set_weights(\n","        [optim_results.position[0], optim_results.position[1]]\n","    )\n","    game.out_layers[training_agent].set_weights(\n","        [optim_results.position[2], optim_results.position[3]]\n","    )\n","\n","    # Evaluation\n","    for i, scenario in enumerate(batch):\n","        if i == 0:\n","            _rewards = game(scenario)\n","        else:\n","            _rewards += game(scenario)\n","    _rewards = _rewards / len(batch)\n","\n","    for agent in np.arange(game.agent_num):\n","        with writer.as_default():\n","            tf.summary.scalar(f\"Average reward {agent}\", _rewards[agent], step=k)\n","\n","\n","if save:\n","    save_weights(\n","        game,\n","        f'saved_model_weights/weights_{datetime.now.strftime(\"%m%d%Y_%H%M%S\")}.txt',\n","    )"],"id":"81d0d542","execution_count":null,"outputs":[{"output_type":"stream","text":[" 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  6.66population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["---------------------------------------\n","Training output:\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","EPOCHS_TOTAL: 0\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","####################### Training agent: 0 ######################\n","Training iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.65population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.16population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.14population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.64population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.45population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.81population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.21population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.68population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.53population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.32population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.81population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.26population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["####################### Training agent: 1 ######################\n","Training iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.71population_element/s]\n","  0%|                                                                            | 0/5 [00:00<?, ?population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.53population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.29population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.75population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.63population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  5.80population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.57population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.40population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.69population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.78population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.22population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.77population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.43population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.02population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.71population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.60population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.50population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.88population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.02population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","EPOCHS_TOTAL: 1\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","####################### Training agent: 0 ######################\n","Training iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.11population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.15population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.25population_element/s]\n","  0%|                                                                            | 0/5 [00:00<?, ?population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.33population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.37population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.54population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.15population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.02population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.96population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.08population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.02population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.20population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.09population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.32population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.57population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.08population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.54population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.61population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.09population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["####################### Training agent: 1 ######################\n","Training iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.14population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.90population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.32population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.68population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.90population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.35population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.43population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.87population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.02population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.07population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.36population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.17population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  8.01population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.57population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.77population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.75population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.71population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.12population_element/s]\n"," 20%|█████████████▌                                                      | 1/5 [00:00<00:00,  7.43population_element/s]"],"name":"stderr"},{"output_type":"stream","text":["Training iteration 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.82population_element/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"af157eab"},"source":["%load_ext tensorboard"],"id":"af157eab","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf78507c","outputId":"80cff9ce-cc5b-4421-d514-34b1aa4caaeb"},"source":["%tensorboard --logdir log/game"],"id":"cf78507c","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-57806a171c147ec9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-57806a171c147ec9\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"133824b9"},"source":[""],"id":"133824b9","execution_count":null,"outputs":[]}]}